{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document summary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moveformyfuture/KRX-Big-Data-Competition/blob/main/Document_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**한국어 문장요약 블로그 **\n",
        "\n",
        "참고 : https://lovit.github.io/nlp/2019/05/01/krwordrank_sentence/"
      ],
      "metadata": {
        "id": "7TuKKAr3FGR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 설치\n",
        "!pip install krwordrank\n",
        "!pip install soynlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_AgHuxO4qOw",
        "outputId": "f332aef8-8bba-4b69-f0be-14d3024ece87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting krwordrank\n",
            "  Downloading krwordrank-1.0.3-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from krwordrank) (1.5.4)\n",
            "Requirement already satisfied: numpy>=1.18.4 in /usr/local/lib/python3.7/dist-packages (from krwordrank) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from krwordrank) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->krwordrank) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->krwordrank) (3.1.0)\n",
            "Installing collected packages: krwordrank\n",
            "Successfully installed krwordrank-1.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting soynlp\n",
            "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
            "\u001b[K     |████████████████████████████████| 416 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.5.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.0.2)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqTWnO3gub3g"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 불러오기\n",
        "import os\n",
        "from pickle import FALSE\n",
        "import re\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import math\n",
        "import pprint \n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from functools import reduce\n",
        "from typing import Text\n",
        "from krwordrank.word import KRWordRank\n",
        "from scipy.sparse import csr_matrix\n",
        "from soynlp.tokenizer import MaxScoreTokenizer\n",
        "\n",
        "# 크롤링으로 수집한 데이터를 전처리하여 필요한 필드를 추가하는 데이터\n",
        "RAW_DATA_DIR = \"..//data//orgin_data//\"\n",
        "PROCESSED_DATA_DIR = \"..//news_article//data//processed//\"\n",
        "rawdata = \"extract_data.xlsx\"\n",
        "\n",
        "class TextMing :\n",
        "    def remove_unicode(input_str):\n",
        "        # 유니코드 :  https://gist.github.com/Pusnow/aa865fa21f9557fa58d691a8b79f8a6d\n",
        "        import unicodedata\n",
        "        try:\n",
        "            input_str = str(input_str)\n",
        "            text = unicode(input_str, 'utf-8')\n",
        "            print(text)\n",
        "            \n",
        "        except (TypeError, NameError): # unicode is a default on python 3 \n",
        "            pass\n",
        "        \n",
        "\n",
        "        text = unicodedata.normalize('NFKC', input_str)\n",
        "        text = text.encode('utf-8','ignore')\n",
        "        text = text.decode(\"utf-8\")\n",
        "            \n",
        "        return text\n",
        "\n",
        "    def clean_ko_sentences(sentences):\n",
        "        sentence = TextMing.remove_unicode(sentences)\n",
        "        # 정규 표현식 필터\n",
        "        RE_FILTER = re.compile(r\"[#▲,<>!?\\\"'·:;=~()]\") # [] 안에 제거할 정규표현식을 추가한다\n",
        "        # 특수기호 제거\n",
        "        sentences = re.sub(RE_FILTER, \"\", sentence)\n",
        "        sentences = re.sub(r\"[^A-Za-z0-9가-힣?!,¿]+\", \" \",sentences) \n",
        "        sentences = re.sub(r\"\\s\",\" \", sentences)# \\n도 공백으로 대체해줌\n",
        "        sentences = sentences.strip()  \n",
        "        \n",
        "        return str(sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Morph_ko_document(sentence_List):\n",
        "    # KoNLPy 형태소분석기 설정 '\n",
        "    from konlpy.tag import Hannanum\n",
        "    hannanum = Hannanum()\n",
        "\n",
        "    nouns = []\n",
        "\n",
        "    for sentence in sentence_List:\n",
        "        sentence = TextMing.clean_ko_documents(sentence)\n",
        "        # print(sentence)\n",
        "        nouns_list = hannanum.nouns(str(sentence))\n",
        "        nouns.append(nouns_list)\n",
        "        # nouns_list =  TextMing.clean_ko_words(nouns_list)\n",
        "\n",
        "    return nouns \n",
        "\n",
        "\n",
        "def extract(self, docs, beta=0.85, max_iter=10, num_keywords=-1,\n",
        "    num_rset=-1, vocabulary=None, bias=None, rset=None):\n",
        "\n",
        "    rank, graph = self.train(docs, beta, max_iter, vocabulary, bias)\n",
        "\n",
        "    lset = {self.int2token(idx)[0]:r for idx, r in rank.items() if self.int2token(idx)[1] == 'L'}\n",
        "    if not rset:\n",
        "        rset = {self.int2token(idx)[0]:r for idx, r in rank.items() if self.int2token(idx)[1] == 'R'}\n",
        "\n",
        "    if num_rset > 0:\n",
        "        rset = {token:r for token, r in sorted(rset.items(), key=lambda x:-x[1])[:num_rset]}\n",
        "\n",
        "    keywords = self._select_keywords(lset, rset)\n",
        "    keywords = self._filter_compounds(keywords)\n",
        "    keywords = self._filter_subtokens(keywords)\n",
        "\n",
        "    if num_keywords > 0:\n",
        "        keywords = {token:r for token, r in sorted(keywords.items(), key=lambda x:-x[1])[:num_keywords]}\n",
        "    \n",
        "    else : \n",
        "        pass\n",
        "\n",
        "    return keywords, rank, graph\n",
        "\n",
        "\n",
        "def make_vocab_score(keywords, scaling=None):\n",
        "    if scaling is None:\n",
        "        scaling = lambda x:math.sqrt(x)\n",
        "    return {word:scaling(rank) for word, rank in keywords.items()}\n",
        "\n",
        "\n",
        "def vectorize(sents, vocab_to_idx, tokenize):\n",
        "    from scipy.sparse import csr_matrix\n",
        "    from soynlp.tokenizer import MaxScoreTokenizer\n",
        "    \n",
        "    rows, cols, data = [], [], []\n",
        "    for i, sent in enumerate(sents):\n",
        "        terms = set(tokenize(sent))\n",
        "        for term in terms:\n",
        "            j = vocab_to_idx.get(term, -1)\n",
        "            if j == -1:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(1)\n",
        "\n",
        "    n_docs = len(sents)\n",
        "    n_terms = len(vocab_to_idx)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_docs, n_terms))\n",
        "\n",
        "\n",
        "def select(x, keyvec, texts, initial_penalty, topk=10):\n",
        "    from sklearn.metrics import pairwise_distances\n",
        "\n",
        "    dist = pairwise_distances(x, keyvec, metric='cosine').reshape(-1)\n",
        "    dist = dist + initial_penalty\n",
        "\n",
        "    idxs = []\n",
        "    for _ in range(topk):\n",
        "        idx = dist.argmin()\n",
        "        idxs.append(idx)\n",
        "        dist[idx] += 2 # maximum distance of cosine is 2\n",
        "        idx_all_distance = pairwise_distances(\n",
        "            x, x[idx].reshape(1,-1), metric='cosine').reshape(-1)\n",
        "        penalty = np.zeros(idx_all_distance.shape[0])\n",
        "        penalty[np.where(idx_all_distance < diversity)[0]] = 2\n",
        "        dist += penalty\n",
        "    return [texts[idx] for idx in idxs]"
      ],
      "metadata": {
        "id": "bTkvJwOy3bRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from krwordrank.sentence import summarize_with_sentences\n",
        "\n",
        "penalty = lambda x:0 if (25 <= len(x) <= 80) else 1\n",
        "stopwords = {'영화', '관람객', '너무', '정말', '진짜'}\n",
        "texts  = []\n",
        "\n",
        "\n",
        "keywords, sents = summarize_with_sentences(\n",
        "      texts,\n",
        "      penalty=penalty,\n",
        "      stopwords = stopwords,\n",
        "      diversity=0.7,\n",
        "      num_keywords=100,\n",
        "      num_keysents=10,\n",
        "      scaling=lambda x:1,\n",
        "      verbose=False,)"
      ],
      "metadata": {
        "id": "-UMdLQZfDvTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#핵심 단어 추출\n",
        "\n",
        "from krwordrank.word import summarize_with_keywords\n",
        "\n",
        "keywords = summarize_with_keywords(texts, min_count=5, max_length=10,\n",
        "    beta=0.85, max_iter=10, stopwords=stopwords, verbose=True)\n",
        "keywords = summarize_with_keywords(texts)"
      ],
      "metadata": {
        "id": "cBYon0HEFkKh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}